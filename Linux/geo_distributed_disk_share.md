Объединение дискового пространства 5 серверов в единое хранилище (250 ГБ)

Для решения задачи объединения пространства 5 разных VPS (по 50 ГБ каждый) в одно сетевое хранилище 250 ГБ целесообразно использовать распределённую файловую систему GlusterFS от Red Hat ￼ ￼. GlusterFS позволит собрать дисковое пространство всех серверов в единый том, доступный на каждом из них (как папка /mnt/vps_shared), распределяя файлы между узлами. Ниже приведена подробная пошаговая инструкция по настройке такой системы.

Шаг 1: Подготовка серверов и установка GlusterFS
	1.	Обновите систему и установите GlusterFS. Выполните на каждом из 5 VPS (Debian 11) установку пакета GlusterFS Server. Это обеспечит запуск демонa glusterd и поддержку работы файловой системы:

sudo apt update && sudo apt install -y glusterfs-server

Команду нужно выполнить на всех пяти серверах (далее назовём их условно server1, server2, …, server5). Пакет glusterfs-server содержит и серверную, и клиентскую часть GlusterFS ￼. После установки запустите и включите службу GlusterFS:

sudo systemctl enable --now glusterd

Убедитесь, что служба запущена (статус active (running)):

sudo systemctl status glusterd


	2.	Настройте сетевые имена/адреса для кластерных узлов. Убедитесь, что все пять серверов могут взаимно подключаться по сети. Если DNS-имён нет, пропишите в файле /etc/hosts на каждом сервере IP-адреса всех узлов с соответствующими именами (например, server1, server2 и т.д.) для удобства обращения. Можно использовать и прямые IP-адреса в командах. Также убедитесь, что брандмауэр (iptables/ufw) не блокирует порты GlusterFS (по умолчанию TCP 24007, 24008 и диапазон 24009-240xx для данных).
	3.	Выделите каталоги под «кирпичи» GlusterFS на каждом сервере. Под «brick» понимается путь на сервере, который GlusterFS будет объединять в единый том. Создайте на каждом узле пустой каталог, который будет служить хранилищем:

sudo mkdir -p /gluster-storage

Примечание: Если 50 ГБ – это размер всего диска сервера, данный каталог будет размещён на основном разделе. Убедитесь, что в этих каталогах достаточно свободного места и они пусты (не содержат никаких данных). В продуктивной среде часто используют отдельные дисковые разделы, отформатированные, например, в XFS или Ext4, специально под GlusterFS-брик. В нашем случае можно обойтись существующим диском, но GlusterFS потребует указать флаг force при создании тома, так как брик находится не на выделенном разделе.

Шаг 2: Объединение серверов в кластер GlusterFS (Trusted Pool)
	4.	Объедините узлы GlusterFS в кластер. Выберите один из серверов (например, server1) и выполните команду peer probe для подключения остальных узлов. Последовательно пробуйте каждый из оставшихся серверов:

gluster peer probe server2
gluster peer probe server3
gluster peer probe server4
gluster peer probe server5

После выполнения каждой команды должна появиться надпись о успешном добавлении пира (peer probe: success). Например, добавление узла server2 к server1 вернёт результат: peer probe: success ￼. Таким образом, server1 узнает о всех других узлах.
Затем на каждом сервере (или хотя бы на одном из них) выполните проверку статуса пиров:

gluster peer status

В списке должны отображаться все 5 узлов со статусом Connected (подключены). Это означает, что все VPS образовали единый доверенный пул (trusted pool) GlusterFS.
Примечание: Достаточно выполнять peer probe с одной стороны – после успешного добавления все узлы становятся известны кластеру. В приведённом примере команда запускалась на server1. Для надёжности можно повторить команду со второго узла, пытаясь добавить server1, чтобы удостовериться, что соединение двустороннее (обычно это не требуется, GlusterFS сам развернёт полносетевое соединение между всеми узлами).

Шаг 3: Создание распределённого тома GlusterFS на 250 ГБ
	5.	Создайте общий том, распределённый по 5 серверам. На одном из узлов (можно на server1) выполните команду создания тома. Мы создадим распределённый (Distributed) том без репликации, чтобы суммировать пространство всех серверов (получив ~250 ГБ). Формат команды:

gluster volume create <имя_тома> <список_блоков(bricks)> [параметры]

В нашем случае:

sudo gluster volume create vps_shared \
    server1:/gluster-storage \
    server2:/gluster-storage \
    server3:/gluster-storage \
    server4:/gluster-storage \
    server5:/gluster-storage \
    force

Здесь vps_shared – название тома (можете задать своё). Перечислены 5 brick-директорий (по одному на каждом сервере). Параметр force указывает, что мы осознанно используем существующие каталоги (не отдельные разделы) ￼. Если команда выполнена успешно, вы увидите сообщение: volume create: <имя_тома>: success.

	6.	Запустите созданный том. Новый том необходимо стартовать перед использованием:

sudo gluster volume start vps_shared

Если запуск успешен, появится подтверждение: volume start: vps_shared: success ￼.

	7.	Проверьте параметры тома (опционально). Выполните команду:

gluster volume info vps_shared

Она должна отобразить информацию о томе vps_shared: тип (Distribute), транспорт (tcp), список всех 5 бриков (сервера и пути) и текущий статус. Также можно использовать gluster volume status vps_shared для просмотра статуса бриков и gluster pool list для списка узлов кластера.
Теперь кластер настроен и общий распределённый том создан. Фактически, файлы, записанные в этот том, будут храниться на разных серверах (каждый файл целиком на одном из узлов, распределение определяется хешированием имен файлов). Общий объём тома составляет ~250 ГБ (сумма свободного места всех узлов), что можно проверить после монтирования с помощью df -h.
Важно: Убедитесь, что размер отдельных файлов не превышает объёма одного сервера (≈50 ГБ), так как в режиме распределения GlusterFS не делит один файл между разными узлами. Например, файл размером 80 ГБ не поместится, потому что не найдётся единичный брик с таким свободным пространством. Однако множество файлов общим объёмом до 250 ГБ разместятся без проблем. Если требуется хранить единичные файлы, превышающие 50 ГБ, нужно рассмотреть другие режимы (например, Striped Volume), но это выходит за рамки текущей задачи.

Шаг 4: Монтирование общего сетевого тома на всех серверах
	8.	Создайте точку монтирования на каждом сервере. На всех 5 VPS подготовьте каталог, куда будет монтироваться общий том GlusterFS:

sudo mkdir -p /mnt/vps_shared

Предполагается, что именно этот путь вы хотели получить в итоге для доступа к объединённому хранилищу.

	9.	Смонтируйте том GlusterFS на каждом сервере. Выполните на каждом узле команду подключения тома:

sudo mount -t glusterfs localhost:/vps_shared /mnt/vps_shared

Здесь мы указываем в качестве сервера localhost (т.е. каждый монтирует через свой локальный demon glusterd). Можно указать и любой узел кластера, например server1:/vps_shared – в процессе монтирования GlusterFS-клиент получит от него конфигурацию кластера. После монтирования выполните df -h и убедитесь, что появился раздел типа glusterfs с точкой монтирования /mnt/vps_shared. Размер этого тома должен отображаться близким к 250 GB, что подтверждает объединение дискового пространства.
Для постоянного монтирования добавьте запись в /etc/fstab на каждом сервере (используйте nano или другой редактор с root-правами):

localhost:/vps_shared /mnt/vps_shared glusterfs defaults,_netdev 0 0

Опция _netdev рекомендуется для сетевых ФС, чтобы система ожидала сеть при автомонтировании. Теперь том будет автоматически монтироваться при перезагрузке сервера ￼.

	10.	Проверьте целостность общего пространства. На данном этапе каждый сервер имеет папку /mnt/vps_shared – это одна и та же распределённая файловая система. Попробуйте на одном из узлов создать файл и убедитесь, что на другом узле в том же каталоге он присутствует:

# на server1:
echo "test" > /mnt/vps_shared/testfile.txt

# на server2:
ls /mnt/vps_shared

Файл testfile.txt должен отобразиться на всех серверах в каталоге /mnt/vps_shared. Хотя физически он записан только на одном из узлов (вы можете заметить, что на каком именно, заглянув в директорию-брик /gluster-storage того узла), GlusterFS обеспечивает единое пространство имён и доступ ко всем файлам через точку монтирования на любом сервере.

Шаг 5: Подключение общей папки внутри LXC-контейнеров (Proxmox VE)

В вашей среде каждый VPS работает под Proxmox VE 8 и запущен контейнер LXC (Ubuntu 22.04), где выполняются Python-скрипты. Чтобы эти контейнеры могли использовать общую папку /mnt/vps_shared, воспользуемся bind-mount из Proxmox, поскольку монтировать FUSE-файловую систему непосредственно внутри LXC не рекомендуется (это требует дополнительных привилегий и может мешать снимкам и бэкапам контейнера) ￼.
	11.	Выполните монтирование на уровне хоста Proxmox. Поскольку мы уже смонтировали GlusterFS-том на каждом хосте (VPS) в каталог /mnt/vps_shared, нам осталось пробросить этот каталог в контейнер. В Proxmox это делается через опцию mp (mount point) в конфигурации контейнера:
	•	Откройте конфигурационный файл контейнера на соответствующем хосте: /etc/pve/lxc/<CTID>.conf, где <CTID> – ID вашего контейнера (например, 101).
	•	Добавьте строчку вида:

mp0: /mnt/vps_shared,mp=/mnt/vps_shared

Это значит, что на уровне контейнера в точке /mnt/vps_shared будет примонтирован каталог хоста /mnt/vps_shared. Можно выполнить то же самое через веб-интерфейс Proxmox (добавить Mount Point) или через команду CLI:

pct set <CTID> -mp0 /mnt/vps_shared,mp=/mnt/vps_shared

Убедитесь, что контейнер остановлен перед изменением настроек. После добавления точки монтирования, запустите контейнер снова.

	12.	Проверьте общую папку внутри контейнера. Зайдите внутрь LXC (через pct enter <CTID> или по SSH) и выполните:

df -h | grep vps_shared
ls -l /mnt/vps_shared

Вы должны увидеть, что каталог /mnt/vps_shared внутри контейнера содержит те же файлы, что и на хосте (и на всех узлах кластера). Теперь ваши скрипты внутри контейнера могут использовать этот путь для записи видеофайлов. Например, убедитесь, что файл, созданный на шаге 10, виден внутри контейнера. Аналогично настройте точки монтирования для контейнеров на всех остальных серверах (каждый Proxmox-хост пробрасывает свой /mnt/vps_shared в контейнер на том же узле).

	13.	Тестирование записи большого объёма данных. Попробуйте запустить ваши Python-скрипты с загрузкой видео в общий каталог на одном из контейнеров. Убедитесь, что при заполнении более 50 ГБ данные продолжают успешно сохраняться (они просто распределятся по разным узлам кластера). Вы можете контролировать загрузку дисков на узлах командой df -h (на хостах, смотря свободное место в /gluster-storage) и наблюдать, как заполняется пространство на разных серверах.

Если все шаги выполнены правильно, теперь у вас есть единое сетевое хранилище 250 ГБ, доступное на всех 5 серверах (и внутри их LXC-контейнеров) через путь /mnt/vps_shared. Вы добились желаемого расширения дискового пространства за счёт объединения ресурсов пяти VPS. Пользуйтесь общей папкой для сохранения данных, зная, что суммарно можете занять до ~250 ГБ, при условии равномерного распределения файлов по узлам кластера.

Источники и дополнительная информация:
	•	Официальная документация GlusterFS (описание типов томов: Distributed, Replicated, Dispersed и др.) ￼.
	•	Пошаговый пример настройки GlusterFS (репликация на 2 узлах) ￼ ￼ ￼ ￼ – на основе него адаптирована наша инструкция под 5 узлов и распределённый (не реплицируемый) том.
	•	Рекомендации сообщества Proxmox по использованию FUSE (GlusterFS) с LXC: монтирование на хосте и bind-mount в контейнер ￼.